{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\nfrom datetime import datetime\nfrom torch import nn, Tensor\nfrom torch.nn import init\nfrom torch.nn.modules.utils import _pair\nfrom torch.nn.parameter import Parameter\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:58:01.099771Z","iopub.execute_input":"2023-12-23T16:58:01.100190Z","iopub.status.idle":"2023-12-23T16:58:01.106921Z","shell.execute_reply.started":"2023-12-23T16:58:01.100158Z","shell.execute_reply":"2023-12-23T16:58:01.105918Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"### Define data transformations\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nbatch_size = 512\n\n### CIFAR-10 train set\ntrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n### Data loader for the train set\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n\n### CIFAR-10 test set\ntest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n### Data loader for the test set\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:58:01.438247Z","iopub.execute_input":"2023-12-23T16:58:01.438613Z","iopub.status.idle":"2023-12-23T16:58:02.992598Z","shell.execute_reply.started":"2023-12-23T16:58:01.438581Z","shell.execute_reply":"2023-12-23T16:58:02.991828Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"### Conventional CNN model\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride = 1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride = 1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10) ,\n            nn.Softmax()\n            )\n        \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:58:02.994058Z","iopub.execute_input":"2023-12-23T16:58:02.994344Z","iopub.status.idle":"2023-12-23T16:58:03.002364Z","shell.execute_reply.started":"2023-12-23T16:58:02.994318Z","shell.execute_reply":"2023-12-23T16:58:03.001440Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"def initialize_weights(model):\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:58:03.003454Z","iopub.execute_input":"2023-12-23T16:58:03.003706Z","iopub.status.idle":"2023-12-23T16:58:03.012316Z","shell.execute_reply.started":"2023-12-23T16:58:03.003684Z","shell.execute_reply":"2023-12-23T16:58:03.011608Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"### Initialize the model and parameters( weights and biases) \ncnn = CNN().to(device)\ninitialize_weights(cnn)\n\n### learning rate and number of epochs\nlearning_rate = 0.001\nnum_epochs = 40\n\n### Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# Training cnn \ntotal_step = len(train_loader)\nstart_time = datetime.now()\nfor epoch in range(num_epochs):\n    cnn.train()\n    running_loss = 0 \n    train_correct = 0\n    train_total = 0\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        # Forward pass\n        outputs = cnn(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        predicted  = torch.argmax(outputs, dim =1)            \n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 50 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Average Loss: {running_loss / i:.4f} , Accuracy train : {(train_correct/train_total) * 100 : .2f}')\n\n    # Testing the model\n    cnn.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs =  cnn(images)\n            predicted  = torch.argmax(outputs, dim =1)            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        print(f' Accuracy test : {100 * correct / total}%')\nend_time = datetime.now()\n\nprint(f\"The whole processing took {end_time - start_time} for a conventional convulutional neural network\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:58:03.253893Z","iopub.execute_input":"2023-12-23T16:58:03.254627Z","iopub.status.idle":"2023-12-23T17:07:19.879759Z","shell.execute_reply.started":"2023-12-23T16:58:03.254596Z","shell.execute_reply":"2023-12-23T17:07:19.878867Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Epoch [1/40], Step [50/98], Average Loss: 2.2071 , Accuracy train :  28.89\nAverage Loss : Accuracy test : 47.65%\nEpoch [2/40], Step [50/98], Average Loss: 1.9834 , Accuracy train :  51.56\nAverage Loss : Accuracy test : 55.2%\nEpoch [3/40], Step [50/98], Average Loss: 1.9124 , Accuracy train :  58.73\nAverage Loss : Accuracy test : 59.33%\nEpoch [4/40], Step [50/98], Average Loss: 1.8782 , Accuracy train :  62.30\nAverage Loss : Accuracy test : 60.26%\nEpoch [5/40], Step [50/98], Average Loss: 1.8506 , Accuracy train :  65.04\nAverage Loss : Accuracy test : 60.46%\nEpoch [6/40], Step [50/98], Average Loss: 1.8383 , Accuracy train :  66.29\nAverage Loss : Accuracy test : 64.14%\nEpoch [7/40], Step [50/98], Average Loss: 1.8241 , Accuracy train :  67.53\nAverage Loss : Accuracy test : 64.27%\nEpoch [8/40], Step [50/98], Average Loss: 1.8103 , Accuracy train :  68.94\nAverage Loss : Accuracy test : 64.43%\nEpoch [9/40], Step [50/98], Average Loss: 1.8037 , Accuracy train :  69.70\nAverage Loss : Accuracy test : 65.3%\nEpoch [10/40], Step [50/98], Average Loss: 1.7908 , Accuracy train :  71.07\nAverage Loss : Accuracy test : 65.98%\nEpoch [11/40], Step [50/98], Average Loss: 1.7784 , Accuracy train :  72.30\nAverage Loss : Accuracy test : 65.11%\nEpoch [12/40], Step [50/98], Average Loss: 1.7736 , Accuracy train :  72.72\nAverage Loss : Accuracy test : 65.6%\nEpoch [13/40], Step [50/98], Average Loss: 1.7642 , Accuracy train :  73.64\nAverage Loss : Accuracy test : 65.68%\nEpoch [14/40], Step [50/98], Average Loss: 1.7613 , Accuracy train :  74.03\nAverage Loss : Accuracy test : 66.78%\nEpoch [15/40], Step [50/98], Average Loss: 1.7534 , Accuracy train :  74.68\nAverage Loss : Accuracy test : 64.63%\nEpoch [16/40], Step [50/98], Average Loss: 1.7523 , Accuracy train :  74.84\nAverage Loss : Accuracy test : 66.58%\nEpoch [17/40], Step [50/98], Average Loss: 1.7459 , Accuracy train :  75.37\nAverage Loss : Accuracy test : 67.49%\nEpoch [18/40], Step [50/98], Average Loss: 1.7397 , Accuracy train :  76.08\nAverage Loss : Accuracy test : 67.77%\nEpoch [19/40], Step [50/98], Average Loss: 1.7375 , Accuracy train :  76.16\nAverage Loss : Accuracy test : 67.51%\nEpoch [20/40], Step [50/98], Average Loss: 1.7240 , Accuracy train :  77.65\nAverage Loss : Accuracy test : 66.13%\nEpoch [21/40], Step [50/98], Average Loss: 1.7251 , Accuracy train :  77.58\nAverage Loss : Accuracy test : 67.66%\nEpoch [22/40], Step [50/98], Average Loss: 1.7174 , Accuracy train :  78.32\nAverage Loss : Accuracy test : 66.69%\nEpoch [23/40], Step [50/98], Average Loss: 1.7135 , Accuracy train :  78.70\nAverage Loss : Accuracy test : 67.87%\nEpoch [24/40], Step [50/98], Average Loss: 1.7098 , Accuracy train :  79.01\nAverage Loss : Accuracy test : 66.69%\nEpoch [25/40], Step [50/98], Average Loss: 1.7053 , Accuracy train :  79.61\nAverage Loss : Accuracy test : 69.0%\nEpoch [26/40], Step [50/98], Average Loss: 1.6804 , Accuracy train :  82.06\nAverage Loss : Accuracy test : 70.49%\nEpoch [27/40], Step [50/98], Average Loss: 1.6745 , Accuracy train :  82.64\nAverage Loss : Accuracy test : 70.07%\nEpoch [28/40], Step [50/98], Average Loss: 1.6656 , Accuracy train :  83.51\nAverage Loss : Accuracy test : 71.06%\nEpoch [29/40], Step [50/98], Average Loss: 1.6633 , Accuracy train :  83.78\nAverage Loss : Accuracy test : 70.24%\nEpoch [30/40], Step [50/98], Average Loss: 1.6519 , Accuracy train :  85.01\nAverage Loss : Accuracy test : 71.76%\nEpoch [31/40], Step [50/98], Average Loss: 1.6455 , Accuracy train :  85.64\nAverage Loss : Accuracy test : 70.98%\nEpoch [32/40], Step [50/98], Average Loss: 1.6505 , Accuracy train :  84.93\nAverage Loss : Accuracy test : 71.97%\nEpoch [33/40], Step [50/98], Average Loss: 1.6384 , Accuracy train :  86.20\nAverage Loss : Accuracy test : 72.0%\nEpoch [34/40], Step [50/98], Average Loss: 1.6308 , Accuracy train :  86.97\nAverage Loss : Accuracy test : 72.12%\nEpoch [35/40], Step [50/98], Average Loss: 1.6281 , Accuracy train :  87.16\nAverage Loss : Accuracy test : 71.41%\nEpoch [36/40], Step [50/98], Average Loss: 1.6278 , Accuracy train :  87.33\nAverage Loss : Accuracy test : 71.79%\nEpoch [37/40], Step [50/98], Average Loss: 1.6171 , Accuracy train :  88.34\nAverage Loss : Accuracy test : 72.18%\nEpoch [38/40], Step [50/98], Average Loss: 1.6242 , Accuracy train :  87.64\nAverage Loss : Accuracy test : 71.07%\nEpoch [39/40], Step [50/98], Average Loss: 1.6139 , Accuracy train :  88.54\nAverage Loss : Accuracy test : 72.35%\nEpoch [40/40], Step [50/98], Average Loss: 1.6098 , Accuracy train :  88.99\nAverage Loss : Accuracy test : 72.8%\nThe whole processing took 0:09:16.607204 for a conventional convulutional neural network\n","output_type":"stream"}]},{"cell_type":"code","source":"def deform_conv2d(\n    input,\n    offset,\n    weight,\n    bias,\n    stride = (1, 1),\n    padding = (0, 0),\n    dilation = (1, 1),\n    mask = None,\n):\n    out_channels = weight.shape[0]\n\n    use_mask = mask is not None\n\n    if mask is None:\n        mask = torch.zeros((input.shape[0], 0), device=input.device, dtype=input.dtype)\n\n    if bias is None:\n        bias = torch.zeros(out_channels, device=input.device, dtype=input.dtype)\n\n    stride_h, stride_w = _pair(stride)\n    pad_h, pad_w = _pair(padding)\n    dil_h, dil_w = _pair(dilation)\n    weights_h, weights_w = weight.shape[-2:]\n    _, n_in_channels, _, _ = input.shape\n\n    n_offset_grps = offset.shape[1] // (2 * weights_h * weights_w)\n    n_weight_grps = n_in_channels // weight.shape[1]\n    \n    return torch.ops.torchvision.deform_conv2d(\n        input,\n        weight,\n        offset,\n        mask,\n        bias,\n        stride_h,\n        stride_w,\n        pad_h,\n        pad_w,\n        dil_h,\n        dil_w,\n        n_weight_grps,\n        n_offset_grps,\n        use_mask,\n    )\n\n\nclass DeformConv2d(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n\n        self.weight = Parameter(\n            torch.empty(out_channels, in_channels // groups, self.kernel_size[0], self.kernel_size[1])\n        )\n\n        self.bias = Parameter(torch.empty(out_channels))\n        self.reset_parameters()\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input, offset, mask = None) :\n        return deform_conv2d(\n            input,\n            offset,\n            self.weight,\n            self.bias,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            mask=mask,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:07:27.000498Z","iopub.execute_input":"2023-12-23T17:07:27.001492Z","iopub.status.idle":"2023-12-23T17:07:27.016563Z","shell.execute_reply.started":"2023-12-23T17:07:27.001456Z","shell.execute_reply":"2023-12-23T17:07:27.015627Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"class DeformableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size = 3,stride=1,padding=1,dilation=1,bias=False):\n        super(DeformableConv2d, self).__init__()\n\n        kernel_size = (kernel_size, kernel_size)\n        \n        self.stride = (stride, stride)\n        self.padding = padding\n        self.dilation = dilation\n        self.offset_conv = nn.Conv2d(in_channels,\n                                     2 * kernel_size[0] * kernel_size[1],\n                                     kernel_size=kernel_size,\n                                     stride=stride,\n                                     padding=self.padding,\n                                     dilation=self.dilation,\n                                     bias=True)\n\n        nn.init.constant_(self.offset_conv.weight, 0.)\n        nn.init.constant_(self.offset_conv.bias, 0.)\n\n        self.modulator_conv = nn.Conv2d(in_channels,\n                                        1 * kernel_size[0] * kernel_size[1],\n                                        kernel_size=kernel_size,\n                                        stride=stride,\n                                        padding=self.padding,\n                                        dilation=self.dilation,\n                                        bias=True)\n\n        nn.init.constant_(self.modulator_conv.weight, 0.)\n        nn.init.constant_(self.modulator_conv.bias, 0.)\n\n        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n                                      out_channels=out_channels,\n                                      kernel_size=kernel_size,\n                                      stride=stride,\n                                      padding=self.padding,\n                                      dilation=self.dilation,\n                                      bias=bias)\n\n    def forward(self, x):\n        offset = self.offset_conv(x)\n        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n        x = deform_conv2d(input=x,\n                                          offset=offset,\n                                          weight=self.regular_conv.weight,\n                                          bias=self.regular_conv.bias,\n                                          padding=self.padding,\n                                          mask=modulator,\n                                          stride=self.stride,\n                                          dilation=self.dilation)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:26:57.799583Z","iopub.execute_input":"2023-12-23T17:26:57.800350Z","iopub.status.idle":"2023-12-23T17:26:57.811747Z","shell.execute_reply.started":"2023-12-23T17:26:57.800314Z","shell.execute_reply":"2023-12-23T17:26:57.810836Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"class DCNN(nn.Module):\n    def __init__(self):\n        super(DCNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            DeformableConv2d(3, 32, kernel_size=3, stride = 1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            DeformableConv2d(32, 64, kernel_size=3, stride = 1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10) ,\n            nn.Softmax()\n            )\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:26:58.607480Z","iopub.execute_input":"2023-12-23T17:26:58.608154Z","iopub.status.idle":"2023-12-23T17:26:58.615776Z","shell.execute_reply.started":"2023-12-23T17:26:58.608120Z","shell.execute_reply":"2023-12-23T17:26:58.614905Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"### Initialize the model and parameters( weights and biases) \ndcnn = DCNN().to(device)\ninitialize_weights(dcnn)\n\n### learning rate and number of epochs\nlearning_rate = 0.001\nnum_epochs = 40\n\n### Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(dcnn.parameters(), lr=learning_rate)\n\n# Training cnn \ntotal_step = len(train_loader)\nstart_time = datetime.now()\nfor epoch in range(num_epochs):\n    dcnn.train()\n    running_loss = 0 \n    train_correct = 0\n    train_total = 0\n    for i, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        # Forward pass\n        outputs = dcnn(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        predicted  = torch.argmax(outputs, dim =1)            \n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i+1) % 50 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Average Loss: {running_loss / i:.4f} , Accuracy train : {(train_correct/train_total) * 100 : .2f}')\n\n    # Testing the model\n    dcnn.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs =  dcnn(images)\n            predicted  = torch.argmax(outputs, dim =1)            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        print(f'Average Loss : Accuracy test : {100 * correct / total}%')\nend_time = datetime.now()\n\nprint(f\"The whole processing took {end_time - start_time} for a Deformable convulutional neural network\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:40:58.669255Z","iopub.execute_input":"2023-12-23T17:40:58.669897Z","iopub.status.idle":"2023-12-23T17:51:03.433117Z","shell.execute_reply.started":"2023-12-23T17:40:58.669863Z","shell.execute_reply":"2023-12-23T17:51:03.432084Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"Epoch [1/40], Step [50/98], Average Loss: 1.8089 , Accuracy train :  69.11\nAverage Loss : Accuracy test : 64.26%\nEpoch [2/40], Step [50/98], Average Loss: 1.7950 , Accuracy train :  70.54\nAverage Loss : Accuracy test : 66.36%\nEpoch [3/40], Step [50/98], Average Loss: 1.7820 , Accuracy train :  71.81\nAverage Loss : Accuracy test : 65.78%\nEpoch [4/40], Step [50/98], Average Loss: 1.7729 , Accuracy train :  72.73\nAverage Loss : Accuracy test : 66.4%\nEpoch [5/40], Step [50/98], Average Loss: 1.7645 , Accuracy train :  73.68\nAverage Loss : Accuracy test : 65.96%\nEpoch [6/40], Step [50/98], Average Loss: 1.7647 , Accuracy train :  73.59\nAverage Loss : Accuracy test : 66.33%\nEpoch [7/40], Step [50/98], Average Loss: 1.7535 , Accuracy train :  74.63\nAverage Loss : Accuracy test : 67.13%\nEpoch [8/40], Step [50/98], Average Loss: 1.7485 , Accuracy train :  75.18\nAverage Loss : Accuracy test : 67.22%\nEpoch [9/40], Step [50/98], Average Loss: 1.7368 , Accuracy train :  76.23\nAverage Loss : Accuracy test : 68.31%\nEpoch [10/40], Step [50/98], Average Loss: 1.7241 , Accuracy train :  77.61\nAverage Loss : Accuracy test : 67.83%\nEpoch [11/40], Step [50/98], Average Loss: 1.7112 , Accuracy train :  78.96\nAverage Loss : Accuracy test : 69.42%\nEpoch [12/40], Step [50/98], Average Loss: 1.7006 , Accuracy train :  79.97\nAverage Loss : Accuracy test : 69.91%\nEpoch [13/40], Step [50/98], Average Loss: 1.6894 , Accuracy train :  80.91\nAverage Loss : Accuracy test : 69.89%\nEpoch [14/40], Step [50/98], Average Loss: 1.6803 , Accuracy train :  81.91\nAverage Loss : Accuracy test : 70.85%\nEpoch [15/40], Step [50/98], Average Loss: 1.6693 , Accuracy train :  83.20\nAverage Loss : Accuracy test : 70.64%\nEpoch [16/40], Step [50/98], Average Loss: 1.6640 , Accuracy train :  83.55\nAverage Loss : Accuracy test : 70.32%\nEpoch [17/40], Step [50/98], Average Loss: 1.6688 , Accuracy train :  83.14\nAverage Loss : Accuracy test : 71.47%\nEpoch [18/40], Step [50/98], Average Loss: 1.6533 , Accuracy train :  84.61\nAverage Loss : Accuracy test : 71.41%\nEpoch [19/40], Step [50/98], Average Loss: 1.6502 , Accuracy train :  85.04\nAverage Loss : Accuracy test : 70.62%\nEpoch [20/40], Step [50/98], Average Loss: 1.6492 , Accuracy train :  84.98\nAverage Loss : Accuracy test : 71.49%\nEpoch [21/40], Step [50/98], Average Loss: 1.6436 , Accuracy train :  85.49\nAverage Loss : Accuracy test : 70.9%\nEpoch [22/40], Step [50/98], Average Loss: 1.6444 , Accuracy train :  85.43\nAverage Loss : Accuracy test : 71.16%\nEpoch [23/40], Step [50/98], Average Loss: 1.6314 , Accuracy train :  86.82\nAverage Loss : Accuracy test : 71.62%\nEpoch [24/40], Step [50/98], Average Loss: 1.6287 , Accuracy train :  87.10\nAverage Loss : Accuracy test : 71.6%\nEpoch [25/40], Step [50/98], Average Loss: 1.6289 , Accuracy train :  87.00\nAverage Loss : Accuracy test : 71.22%\nEpoch [26/40], Step [50/98], Average Loss: 1.6211 , Accuracy train :  87.70\nAverage Loss : Accuracy test : 71.36%\nEpoch [27/40], Step [50/98], Average Loss: 1.6179 , Accuracy train :  88.02\nAverage Loss : Accuracy test : 71.08%\nEpoch [28/40], Step [50/98], Average Loss: 1.6185 , Accuracy train :  88.12\nAverage Loss : Accuracy test : 71.45%\nEpoch [29/40], Step [50/98], Average Loss: 1.6159 , Accuracy train :  88.26\nAverage Loss : Accuracy test : 71.24%\nEpoch [30/40], Step [50/98], Average Loss: 1.6134 , Accuracy train :  88.45\nAverage Loss : Accuracy test : 71.52%\nEpoch [31/40], Step [50/98], Average Loss: 1.6090 , Accuracy train :  88.93\nAverage Loss : Accuracy test : 71.65%\nEpoch [32/40], Step [50/98], Average Loss: 1.6076 , Accuracy train :  89.05\nAverage Loss : Accuracy test : 71.53%\nEpoch [33/40], Step [50/98], Average Loss: 1.6064 , Accuracy train :  89.12\nAverage Loss : Accuracy test : 72.37%\nEpoch [34/40], Step [50/98], Average Loss: 1.6093 , Accuracy train :  88.77\nAverage Loss : Accuracy test : 72.04%\nEpoch [35/40], Step [50/98], Average Loss: 1.6096 , Accuracy train :  88.77\nAverage Loss : Accuracy test : 71.45%\nEpoch [36/40], Step [50/98], Average Loss: 1.5989 , Accuracy train :  89.82\nAverage Loss : Accuracy test : 72.08%\nEpoch [37/40], Step [50/98], Average Loss: 1.6002 , Accuracy train :  89.67\nAverage Loss : Accuracy test : 71.79%\nEpoch [38/40], Step [50/98], Average Loss: 1.5976 , Accuracy train :  89.93\nAverage Loss : Accuracy test : 71.43%\nEpoch [39/40], Step [50/98], Average Loss: 1.5985 , Accuracy train :  89.80\nAverage Loss : Accuracy test : 71.33%\nEpoch [40/40], Step [50/98], Average Loss: 1.5965 , Accuracy train :  90.03\nAverage Loss : Accuracy test : 71.57%\nThe whole processing took 0:10:04.750590 for a Deformable convulutional neural network\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}